\documentclass[10pt,conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment

\begin{document}
\title{Project 1 - finding the Higgs Boson}

\author{
  L\'ea Bommottet, Nicolas Brunner, Karine Perrard\\
  \textit{CS-433 Machine Learning, EPFL, Switzerland}
}

\maketitle

\section{Introduction}

In this project we have to solve a binary classification problem. In function of a vector of features regarding a proton collision event, we have to determine if this collision produced an Higgs boson or another process,
there is at most 30 parameters provided for each classification. We have to implement and use machine learning methods, then feature processing and engineering to solve this problem.

\section{Models and methods}

\subsection{Choice of Models}
First of all, we have to select the core of our model. In this project, there is 6 possible models on which we can base.
\subsubsection{Linear regression using gradient descent}
	
	This model try to minimize the error of the function, by moving the vector w in the reverse trajectory of the gradient.
	$$w^{(t+1)}=w^{(t)} -\gamma \nabla L(w^{(t)})$$
	$$e = y-Xw$$
	$$\nabla L(w) = -\frac{1}{N}X^te$$
	
\subsubsection{Linear regression using stochastic gradient descent}
	
	The error function is defined as a sum over the training errors: \\
    $$ L(w) = \sum\limits_{n=1}^N L_n(w)$$
    As for gradient descent, we want to follow the gradient, but this time the updating step is done on the cost contributed by one of the training examples. We can also compute the gradient on a subset of the examples, this is mini-batch stochastic gradient descent.
    $$w^{(t+1)}=w^{(t)} -\gamma \nabla L_n(w^{(t)})$$
	
	
\subsubsection{Least squares regression using normal equations}
	
	We try to solve: $\nabla L(w^*) = 0$
	
	Then we derive
	$$w^* = (X^\top X)^{-1}X^\top y$$
	and a unknown data-point would have 
	$$\hat y_m := x_m^\top w^*$$
	
\subsubsection{Ridge regression using normal equations}
	
	We want to punish complex models and conversely choose simpler ones.
	In the case of ridge regression, we add a regularizer $\Omega(w) = \lambda||w||_2^2$ in the quest to minimize w:
	$$\min\limits_{w} L(w) + \Omega(w)$$
	the explicit solution of w become:
	$$w_{ridge}^* = (X^\top X+2N\lambda I)^{-1}X^\top y$$
\subsubsection{Logistic regression using gradient descent or SGD}
	
    The problem is about separating the outputs into different labels. To do so we use the logitic function, that gives values in the range [0, 1]:
    $$\sigma(z) := \frac{e^z}{1 + e^z}$$
	Again we want to minimize the cost function defined as 
    $$ L(w) = \sum\limits_{n=1}^N ln[1 + e^{x_n^Tw}] - y_nx_n^Tw$$
    To compute the model, we use gradient descent. The formula is 
    $$w^{(t+1)}=w^{(t)} -\gamma \nabla L(w^{(t)})$$
    where  $$\nabla L(w) = X^T[\sigma(Xw) - y]$$ 
   
\subsubsection{Regularized logistic regression using gradient descent or SGD}
	We can have troubles if the data is linearly separable. In that case, any vector \textbf{w} having the right direction is solution gives the minimum for \textbf{L(w)}. The goal of regularization is to add a penalty term, which is the length of the vector.
    $$ w* = argmin_w - \sum\limits_{n=1}^N ln p(y_n|x_n^T, w) + \frac{\lambda}{2}||w||^2 $$


The reason we based our model on logistic regression is because we have the least global error on the dataset. Fig.?? 

Furthermore, in function of the feature named "fit", some parameters are unavailable. Thanks to this important information, we don't train only one model, but 4, since "fit" can be {0,1,2,3}.
This idea reduce our global error, when compared to an single model without this separation. Fig.~\ref{fig:noSepFunction}

\begin{figure}[tbp]
  \centering
  \includegraphics[width=\columnwidth]{valid_train_error_with_thresh_no_jet.png}
  \caption{Train and Validation error with no separation according to number of iterations}
  \vspace{-3mm}
  \label{fig:noSepFunction}
\end{figure}

\subsection{Choice of parameters}
Blablabla
Cross-correlation by K-fold:

As seen in Fig.??, the parameters that reduces most the train error are lambda = ??? and gamma = ???.

\subsection{Choice of features}

We can improve our model by adding new features, which are function of other parameters. We want more features that have a Normal distribution. 
\begin{description}
	\item[Natural logarithm] \ \\
	Applying the logarithm on a parameter can highlight its Normal behavior. One example is this function on the feature 21 (Fig.~\ref{fig:feature21function}). In this case, we notice a much nicer Gaussian distribution on the values, than the initial data. By doing an exhaustive research, we find that the useful parameters with this function are \{0, 1, 2, 3, 4, 5, 8, 9, 10, 13, 16, 19, 21, 23, 26, 29\}, when discarding the NaN value.
	\item[Square root] \ \\
	Same as the natural logarithm, the square root can bring out a normal distribution. The feature 21 is also transformed with this function in our model (Fig.~\ref{fig:feature21function}). Again if we don't consider the NaN value, the parameters \{0, 13, 16, 21, 23, 26, 29\} have a Normal behavior under the square root
	\item[Threshold] \ \\
	Some features are mostly distributed along two peaks (Fig.~\ref{fig:feature11}). In that case we move values to their closest peak to get two well defined peaks. This is the case for parameters \{11, 12\}.
	\item[Nothing max] \ \\
	Some features have no defined distribution, but their values explode to big numbers. In that case we divide by maximum to get new value in range [0, 1]. Such distributed parameters are parameters \{6, 14, 17, 24, 27\}
	\item[Nothing norm] \ \\
	Some features have no distribution and their values are already between 0 and 1. For parameter 7 we do nothing.
	\item[Distance] \ \\
	When we plot 2 parameters in a graph, we can find some correlation between (Fig.~\ref{fig:feature1825}). To reduces this property to a normal distribution, we compute the Manhattan distance, to flatten all the perpendicular result on the plot $y = -x$. The Manhattan distance is computed as follow:
    $$dist(x_1, x_2) = |x_1 - x_2|$$ 
    
	\item[$N^{th}$ power] \ \\
	Some feature sometimes have a better behavior when they are raised to a power greater than 2, as it is the case for feature 19 when raise to the cube (Fig.~\ref{fig:feature19})
\end{description}
\begin{figure}[tbp]
  \centering
  \includegraphics[width=\columnwidth]{21_0}
  \caption{Function applied on the data of the feature 21}
  \vspace{-3mm}
  \label{fig:feature21function}
\end{figure}
\begin{figure}[tbp]
  \centering
  \includegraphics[width=\columnwidth]{feature11}
  \caption{Distribution for feature 11}
  \vspace{-3mm}
  \label{fig:feature11}
\end{figure}
\begin{figure}[tbp]
  \centering
  \includegraphics[width=\columnwidth]{features18_25}
  \caption{Repartition of features 18 ans 25}
  \vspace{-3mm}
  \label{fig:feature1825}
\end{figure}
\begin{figure}[tbp]
  \centering
  \includegraphics[width=\columnwidth]{19_3}
  \caption{Function applied to feature 19}
  \vspace{-3mm}
  \label{fig:feature19}
\end{figure}
\section{Results}
On compare entre model simple et le 4 jets.

On montre les graph features, pourquoi certaines sont sqrt, del, cube.

On compare avec et sans ces features en plus.

On montre notre erreur global, on parle de kaggle?



\section{Summary}

Separating our model into 4 different ones was a great idea, reducing considerably our global error (Fig.??) in the same way that including more parameters (Fig.??). Those added criterion improve the quality of our model, adjusting from ??? to ???.    
	0.18\% of error is kinda ok with the simplicity of the core models. If we want to improve it more, we would need to find more useful parameters to boost our data.
    
\bibliographystyle{IEEEtran}
\bibliography{literature}

\end{document}
